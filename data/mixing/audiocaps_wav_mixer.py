# This file is used to mix the audiocaps dataset using its mix.csv file which can then be loaded as an AudioTextMixDataset.
# It does so by processing the AudioCaps .csv file.
# The AudioCaps mix.csv is generated by parsing the original audiocaps csv files using script mixing/audiocaps_csv_mixer.py,
# and creates a .csv containing the following columns:
#
# audiocap_id_target,audiocap_id_interferer,youtube_id_target,youtube_id_interferer,start_time_target,start_time_interferer,caption_target,caption_interferer,caption_similarity
# 66688,20637,_8DVd3NKuTA,7i2gPewuazM,30,130,"A person laughs, followed by a baby laughing, after which both a person and a baby laugh",Beeps and buzzing is going off while a man speaks and starts an engine,0.1799960881471634
#

import argparse
import os
import json
import pandas as pd
import torchaudio

from tqdm import tqdm

from data.mixing.waveform_mixer import WaveformMixer


def premix_audiocaps(csv_file, audiocaps_wav_path, mix_wav_path, output_json,
                     sr=32000, skip_existing=False):
    # Read the csv file
    df = pd.read_csv(csv_file)
    df = df.sort_values(by='audiocap_id_target')

    data = []

    waveform_mixer = WaveformMixer()
    os.makedirs(mix_wav_path, exist_ok=True)

    n_found = n_not_found = 0
    for index, row in tqdm(df.iterrows(), total=len(df)):
        # Create the dictionary
        path_target = os.path.join(
                audiocaps_wav_path, f"{row['audiocap_id_target']}.wav")
        path_interferer = os.path.join(
                audiocaps_wav_path, f"{row['audiocap_id_interferer']}.wav")
        out_mix_path = os.path.join(
                mix_wav_path,
                f"{row['audiocap_id_target']}_{row['audiocap_id_interferer']}.wav")
        # Need to save the target and interferer as they are cropped to 5 seconds
        out_target_path = os.path.join(
                mix_wav_path,
                f"{row['audiocap_id_target']}.wav")
        out_interferer_path = os.path.join(
                mix_wav_path,
                f"{row['audiocap_id_interferer']}.wav")

        if not os.path.exists(path_target):
            print(f"{row['audiocap_id_target']} wav file not found {n_found}/{n_not_found}")
            n_not_found +=1
            continue
        if not os.path.exists(path_interferer):
            print(f"{row['audiocap_id_interferer']} wav file not found {n_found}/{n_not_found}")
            n_not_found += 1
            continue

        n_found += 1
        data_dict = {
            "wav_mixture": out_mix_path,
            "wav_target": out_target_path,
            "wav_interferer": out_interferer_path,
            "caption_target": row["caption_target"],
            "caption_interferer": row["caption_interferer"],
        }
        data.append(data_dict)

        if os.path.exists(out_mix_path) and skip_existing:
            # print(f"Skipping row {index} as mix wav file already exists")
            continue

        wav_target = _load_audio(path_target, sr)
        wav_interferer = _load_audio(path_interferer, sr)

        mixture, target, interferers  = waveform_mixer.mix(
            wav_target,
            [wav_interferer],
            return_tracks=True
        )

        torchaudio.save(out_mix_path, mixture, sr)
        torchaudio.save(out_target_path, target, sr)
        torchaudio.save(out_interferer_path, interferers[0], sr)

    # Write the dictionary to a json file
    with open(output_json, 'w') as f:
        json.dump({ "data": data}, f, indent=4)
    
    print(f"Template json file created at {output_json}")


def _load_audio(path, target_sr):
    audio, sr = torchaudio.load(path, channels_first=True)
    if audio.shape[0] > 1:
        # audio: [samples]
        audio = (audio[0] + audio[1]) / 2
    else:
        audio = audio.squeeze(0)

    if sr != target_sr:
        audio = torchaudio.functional.resample(audio, orig_freq=sr, new_freq=target_sr)
        
    return audio


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a template json file for the audiocaps dataset")
    parser.add_argument("--in_csv_dir", type=str, required=True, help="Path to the directory containing the files train.csv, val.csv, test.csv")
    parser.add_argument("--in_wav_dir", type=str, required=True, help="Path to the audiocaps wav files")
    parser.add_argument("--out_wav_dir", type=str, required=True, help="Path to the audiocaps wav files")
    parser.add_argument("--out_json_dir", type=str, required=True, help="Path to the directory where the .json files will be saved")
    args = parser.parse_args()
    
    for split in ["train", "val", "test"]:
        in_csv_file = os.path.join(args.in_csv_dir, f"audiocaps_{split}_mix.csv")
        output_json = os.path.join(args.out_json_dir, f"audiocaps_{split}_mix.json")
        wav_dir = os.path.join(args.in_wav_dir, split)
        mix_wav_dir = os.path.join(args.out_wav_dir, split)
        print(f"Creating template json file for {split} split")
        premix_audiocaps(in_csv_file, wav_dir, mix_wav_dir, output_json)